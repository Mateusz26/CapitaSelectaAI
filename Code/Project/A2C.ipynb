{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to host 127.0.0.1 at port 49533 ...\n",
      "Client connected to server [OK]\n",
      "Connecting to host 127.0.0.1 at port 49536 ...\n",
      "Client connected to server [OK]\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x146731050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x146731050>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x146731050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x146731050>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14706a150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14706a150>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14706a150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14706a150>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "---------------------------------\n",
      "| explained_variance | 0.0552   |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.39     |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 0.629    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500, episode_reward=42.00 +/- 15.14\n",
      "Episode length: 547.00 +/- 277.81\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 4        |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 1.39     |\n",
      "| total_timesteps    | 500      |\n",
      "| value_loss         | 0.129    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=38.00 +/- 5.83\n",
      "Episode length: 399.60 +/- 64.20\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 1.39     |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 1.54     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=36.20 +/- 11.60\n",
      "Episode length: 393.40 +/- 109.20\n",
      "---------------------------------\n",
      "| explained_variance | 1.19e-07 |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 1.39     |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 0.000434 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=41.00 +/- 2.00\n",
      "Episode length: 431.60 +/- 32.80\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 1.39     |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 0.00076  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=35.00 +/- 14.00\n",
      "Episode length: 383.80 +/- 128.40\n",
      "----------------------------------\n",
      "| explained_variance | -1.19e-07 |\n",
      "| fps                | 5         |\n",
      "| nupdates           | 500       |\n",
      "| policy_entropy     | 1.39      |\n",
      "| total_timesteps    | 2500      |\n",
      "| value_loss         | 0.00063   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=39.80 +/- 4.40\n",
      "Episode length: 422.00 +/- 52.00\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 1.38     |\n",
      "| total_timesteps    | 3000     |\n",
      "| value_loss         | 0.000834 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=38.40 +/- 5.82\n",
      "Episode length: 409.20 +/- 62.01\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 1.33     |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 0.00312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=34.00 +/- 13.64\n",
      "Episode length: 371.00 +/- 124.49\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 1.2      |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 0.00198  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.20 +/- 11.60\n",
      "Episode length: 393.20 +/- 109.60\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 1.16     |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 0.00214  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=38.60 +/- 6.80\n",
      "Episode length: 412.40 +/- 71.20\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 1.25     |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 2        |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=32.20 +/- 8.61\n",
      "Episode length: 345.00 +/- 87.80\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 1.13     |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 0.00283  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=41.00 +/- 2.00\n",
      "Episode length: 435.20 +/- 25.60\n",
      "----------------------------------\n",
      "| explained_variance | -1.19e-07 |\n",
      "| fps                | 5         |\n",
      "| nupdates           | 1200      |\n",
      "| policy_entropy     | 1.14      |\n",
      "| total_timesteps    | 6000      |\n",
      "| value_loss         | 0.000318  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=42.00 +/- 0.00\n",
      "Episode length: 448.00 +/- 0.00\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 1.29     |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 0.502    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=42.00 +/- 0.00\n",
      "Episode length: 448.00 +/- 0.00\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 1.25     |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 0.00272  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.60 +/- 10.80\n",
      "Episode length: 396.80 +/- 102.40\n",
      "---------------------------------\n",
      "| explained_variance | 1.19e-07 |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 1.21     |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 0.164    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=36.20 +/- 11.60\n",
      "Episode length: 393.40 +/- 109.20\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 1.24     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 8.47e-05 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=8500, episode_reward=35.40 +/- 13.20\n",
      "Episode length: 387.20 +/- 121.60\n",
      "---------------------------------\n",
      "| explained_variance | 0        |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 1.17     |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 0.00324  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import gym_gvgai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import ActorCriticPolicy\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines import SAC\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import TRPO\n",
    "\n",
    "\n",
    "def show_state(env, step=0, name=\"\", info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (name,step,info))\n",
    "    plt.axis('off')\n",
    "              \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    \n",
    "env = gym.make('gvgai-aliens-lvl0-v0')\n",
    "    \n",
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/',\n",
    "log_path='./logs/', eval_freq=500,\n",
    "deterministic=True, render=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('gvgai-aliens-lvl0-v0')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=10000,callback=eval_callback)\n",
    "model.save(\"A2C_aliens\")\n",
    "\n",
    "        \n",
    "env.close()\n",
    "\n",
    "env = gym.make('gvgai-aliens-lvl1-v0')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=10000,callback=eval_callback)\n",
    "model.save(\"A2C_aliens\")\n",
    "        \n",
    "env.close()\n",
    "\n",
    "env = gym.make('gvgai-aliens-lvl2-v0')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=10000,callback=eval_callback)\n",
    "model.save(\"A2C_aliens\")\n",
    "\n",
    "\n",
    "        \n",
    "env.close()\n",
    "\n",
    "env = gym.make('gvgai-aliens-lvl3-v0')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=10000,callback=eval_callback)\n",
    "model.save(\"A2C_aliens\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
